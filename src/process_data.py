import os
import pandas as pd
import model_loader
import logging
import torch

from prompts import prompt_compression
# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)


def compress_article(origin_file: str, processed_file: str):
    # íŒŒì¼ ê²½ë¡œ í™•ì¸
    if os.path.exists(processed_file):
        logging.warning(f"'{processed_file}' íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ë®ì–´ì“°ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return

    logging.info(f"ì…ë ¥ íŒŒì¼ '{origin_file}'ì„(ë¥¼) ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.")
    try:
        df = pd.read_csv(origin_file)
        logging.info(f"CSV íŒŒì¼ ë¡œë“œ ì„±ê³µ: {df.shape[0]}ê°œì˜ í–‰, {df.shape[1]}ê°œì˜ ì—´")
    except Exception as e:
        logging.exception("CSV íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
        raise

    # ì¸ì¦ ë§ˆí¬ ì»¬ëŸ¼ ì¶”ê°€
    if 'en_mark' not in df.columns:
        logging.error("CSV íŒŒì¼ì— 'en_mark' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ì¸ì¦ ë§ˆí¬ ì •ë³´ë¥¼ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
        raise ValueError("CSV íŒŒì¼ì— 'en_mark' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    df["certification_type"] = df["en_mark"].apply(normalize_cert_type)
    logging.info("ì¸ì¦ ë§ˆí¬ ì»¬ëŸ¼ ì¶”ê°€ ì™„ë£Œ")
    logging.info(f"ì¸ì¦ ë§ˆí¬ ì¢…ë¥˜: {df['certification_type'].unique()}")
    logging.info(f"ìƒìœ„ 5ê°œ ì¸ì¦ ë§ˆí¬:\n{df['certification_type'].head(5)}")

    # ëª¨ë¸ ë¡œë“œí•˜ê¸° (llama3ko)
    try:
        logging.info("ê¸°ì‚¬ ì••ì¶•ì— ì‚¬ìš©í•  LLM ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.")
        model, tokenizer = model_loader.llama3Ko_loader()
        tokenizer.pad_token = tokenizer.eos_token
        logging.info("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        logging.exception("ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
        raise

    # í”„ë¡¬í”„íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸° (ê¸°ì‚¬ ì••ì¶• í”„ë¡¬í”„íŠ¸)
    try:
        prompt_template = prompt_compression.base_prompt
        logging.info("í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        logging.exception("í”„ë¡¬í”„íŠ¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
        raise

    compressed_results = []

    for idx, row in df.iterrows():
        try:
            news = row['full_text']
            if pd.isna(news):
                logging.warning(f"âš ï¸  [WARNING] ê¸°ì‚¬ ë‚´ìš©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. (ì¸ë±ìŠ¤: {idx})")
                compressed_results.append("ê¸°ì‚¬ ë‚´ìš© ì—†ìŒ")
                continue

            if len(news) < 700:
                compressed_results.append(news)
                continue

            logging.info("\n" + "=" * 80)
            logging.info(f"ğŸ“„ [ê¸°ì‚¬ {idx + 1}/{len(df)}] ì‹œì‘")
            logging.info(f"ğŸ“ [ê¸°ì‚¬ ê¸¸ì´] {len(news)}ì")
            logging.info(f"ğŸ” [ì›ë¬¸ ì¼ë¶€] {news[:200]}...")
            logging.info("-" * 80)

            prompt = prompt_template.format(news=news)
            messages = [
                {
                    "role": "system",
                    "content": "ë„ˆëŠ” í™˜ê²½ ê¸°ì‚¬ ìš”ì•½ ì „ë¬¸ê°€ì•¼. ì£¼ì–´ì§„ ê¸°ì‚¬ì—ì„œ í™˜ê²½ ê´€ë ¨ ë§ˆì¼€íŒ… ì£¼ì¥ê³¼ ê·¼ê±° ë¬¸ì¥ë§Œ ì¶”ì¶œí•´ 5ë¬¸ì¥ ì´ë‚´ë¡œ ìš”ì•½í•´."
                },
                {"role": "user", "content": prompt}
            ]

            chat_prompt = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )

            inputs = tokenizer(
                chat_prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=tokenizer.model_max_length,
            ).to(model.device)

            with torch.no_grad():
                output_ids = model.generate(
                    input_ids=inputs["input_ids"],
                    attention_mask=inputs["attention_mask"],
                    max_new_tokens=512,
                    temperature=0.5,
                    top_p=0.8,
                    do_sample=True,
                    repetition_penalty=1.15,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id
                )

            compressed = tokenizer.decode(
                output_ids[0], skip_special_tokens=True)

            logging.info("âœ… [ì••ì¶• ê²°ê³¼]")
            logging.info(compressed.strip())
            logging.info("=" * 80)

            compressed_results.append(extract_summary_only(compressed))

        except Exception as e:
            logging.exception(f"âŒ [ì˜¤ë¥˜] {idx+1}ë²ˆì§¸ ê¸°ì‚¬ ì••ì¶• ì¤‘ ì˜ˆì™¸ ë°œìƒ")
            compressed_results.append("ì••ì¶• ì‹¤íŒ¨")

    df['compressed_article'] = compressed_results

    output_file = processed_file
    try:
        df.to_csv(output_file, index=False)
        logging.info(f"ì••ì¶• ê²°ê³¼ê°€ '{output_file}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. (ì´ {len(df)}ê°œ ê¸°ì‚¬)")
    except Exception as e:
        logging.exception("ê²°ê³¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
        raise


def extract_summary_only(full_output: str) -> str:
    """
    ì „ì²´ ì¶œë ¥ ì¤‘ '[ìš”ì•½ëœ í™˜ê²½ ê´€ë ¨ í•µì‹¬ ë¬¸ì¥]' ì´í›„ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
    """
    split_token = "[ìš”ì•½ëœ í™˜ê²½ ê´€ë ¨ í•µì‹¬ ë¬¸ì¥]assistant"
    if split_token in full_output:
        return full_output.split(split_token, 1)[-1].strip()
    return full_output.strip()  # fallback


def normalize_cert_type(text):
    if not isinstance(text, str):
        return "ì—†ìŒ"
    if "íƒ„ì†Œë°œìêµ­" in text:
        return "íƒ„ì†Œë°œìêµ­"
    elif "ì—ë„ˆì§€" in text:
        return "ì—ë„ˆì§€ì ˆì•½"
    elif "í™˜ê²½í‘œì§€" in text or "í™˜ê²½ë¶€ ì¸ì¦" in text:
        return "í™˜ê²½í‘œì§€"
    elif "ì—†ìŒ" in text:
        return "ì—†ìŒ"
    else:
        return "ê¸°íƒ€"
